{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtJwvDlXH1PxWsEi2wtnHu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fmezacr/machinelearning/blob/main/Introducci%C3%B3n_al_Aprendizaje_Profundo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "<div style=\"font-size:small;\">\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) [2024] Felipe Meza-Obando\n",
        "\n",
        "*Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software for educational purposes only. You must give author appropriate credit, provide a link to the license and source, and indicate if changes were made.*\n",
        "</div>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0Xdniryb6uB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introducción al Aprendizaje Profundo (Deep Learning)**\n",
        "### Autor: ***Felipe Meza-Obando***\n",
        "\n",
        "### 1. **Introducción**\n",
        "\n",
        "El aprendizaje profundo (Deep Learning) es una subárea del aprendizaje automático que se centra en el uso de redes neuronales artificiales con múltiples capas para modelar datos complejos. A medida que las arquitecturas de redes neuronales han evolucionado, también lo han hecho las herramientas y técnicas que facilitan su entrenamiento. Librerías como **TensorFlow**, **PyTorch**, y **Keras** han permitido a los investigadores y desarrolladores construir, entrenar y desplegar modelos de aprendizaje profundo de manera eficiente.\n",
        "\n",
        "El propósito de esta introducción es brindar una visión general de las principales librerías utilizadas en el aprendizaje profundo, los desafíos asociados con el entrenamiento de redes profundas y el papel crítico que juega el **gradiente descendente** en este proceso.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Librerías Principales para Aprendizaje Profundo**\n",
        "\n",
        "Existen varias librerías y frameworks que han hecho más accesible el desarrollo de modelos de aprendizaje profundo. Algunas de las más importantes son:\n",
        "\n",
        "#### 2.1 **TensorFlow**\n",
        "TensorFlow, desarrollado por Google, es una de las librerías más populares para el aprendizaje profundo. Proporciona una infraestructura escalable que permite entrenar modelos tanto en CPU como en GPU. TensorFlow es altamente modular y ofrece capacidades para construir desde modelos simples hasta redes neuronales complejas.\n",
        "\n",
        "**Ventajas**:\n",
        "- Alta flexibilidad para construir cualquier tipo de red neuronal.\n",
        "- Excelente soporte para entrenamiento distribuido y deployment (TensorFlow Serving).\n",
        "- Fuerte integración con Keras para facilidad de uso.\n",
        "\n",
        "**Desventajas**:\n",
        "- Curva de aprendizaje relativamente empinada.\n",
        "- Puede ser excesivo para tareas más simples.\n",
        "\n",
        "#### 2.2 **Keras**\n",
        "Keras es una API de alto nivel desarrollada inicialmente para facilitar el uso de TensorFlow. Su simplicidad ha permitido que muchos desarrolladores la adopten como una herramienta preferida para prototipar modelos.\n",
        "\n",
        "**Ventajas**:\n",
        "- Simplicidad y facilidad de uso.\n",
        "- Buena integración con TensorFlow y facilidad para implementar modelos estándar.\n",
        "- Prototipado rápido y eficiente.\n",
        "\n",
        "**Desventajas**:\n",
        "- Menor flexibilidad en comparación con frameworks como PyTorch.\n",
        "- No es ideal para proyectos altamente personalizados.\n",
        "\n",
        "#### 2.3 **PyTorch**\n",
        "PyTorch, desarrollado por Facebook, es otra librería popular que permite crear modelos de aprendizaje profundo de manera intuitiva y eficiente. A diferencia de TensorFlow, PyTorch ofrece una construcción dinámica de gráficos computacionales, lo que lo convierte en una opción preferida para investigación y desarrollo.\n",
        "\n",
        "**Ventajas**:\n",
        "- Soporte para gráficos computacionales dinámicos.\n",
        "- Comunidad activa y excelente documentación.\n",
        "- Más intuitivo para los que provienen de un contexto de programación en Python.\n",
        "\n",
        "**Desventajas**:\n",
        "- Menor soporte nativo para deployment en producción (aunque se ha mejorado con PyTorch Lightning y TorchServe).\n",
        "- Puede ser más complejo para quienes están acostumbrados a APIs de alto nivel.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RAW2d0zx4Qos"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaUWUfcS4P1e",
        "outputId": "db075fb4-82d7-42a0-d3ee-ec99b7a7a747"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Versión de TensorFlow: 2.17.0\n",
            "Versión de PyTorch: 2.4.1+cu121\n"
          ]
        }
      ],
      "source": [
        "# Importar librerías comunes para el desarrollo de aprendizaje profundo\n",
        "\n",
        "# Importar TensorFlow y Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential # clase que permite crear modelos de redes neuronales secuenciales en Keras\n",
        "from tensorflow.keras.layers import Dense # clase que permite crear una capa completamente conectada (fully connected layer)\n",
        "\n",
        "# Importar PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Verificar versiones de las librerías\n",
        "print(f\"Versión de TensorFlow: {tf.__version__}\")\n",
        "print(f\"Versión de PyTorch: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. **Desafíos del Aprendizaje Profundo**\n",
        "\n",
        "El desarrollo y entrenamiento de modelos de redes neuronales profundas enfrenta varios desafíos únicos. A continuación, se describen algunos de los más importantes:\n",
        "\n",
        "#### 3.1. **Sobrecarga Computacional**\n",
        "Uno de los principales retos del aprendizaje profundo es la necesidad de grandes cantidades de poder computacional. Las redes neuronales profundas, especialmente las convolucionales o recurrentes, pueden tardar horas o incluso días en entrenarse, dependiendo de la cantidad de datos y la complejidad de la arquitectura.\n",
        "\n",
        "Las GPU (Unidades de Procesamiento Gráfico) y los **TPUs** (Unidades de Procesamiento Tensor) han surgido como herramientas fundamentales para acelerar estos procesos, proporcionando la capacidad de entrenar modelos en paralelo.\n",
        "\n",
        "#### 3.2. **Gran Cantidad de Datos**\n",
        "Los modelos de aprendizaje profundo necesitan **grandes cantidades de datos** para generalizar correctamente. Los modelos entrenados con conjuntos de datos pequeños corren el riesgo de **sobreajustar** (overfitting), donde el modelo memoriza en lugar de aprender patrones generales en los datos.\n",
        "\n",
        "**Técnicas de regularización** como el **dropout**, **L2 regularization** y **data augmentation** pueden ayudar a mitigar el sobreajuste, pero no son una solución definitiva a la falta de datos.\n",
        "\n",
        "#### 3.3. **Gradiente Desvanecido y Gradiente Explosivo**\n",
        "Estos problemas surgen principalmente en redes neuronales recurrentes (RNNs) o redes neuronales muy profundas. Cuando el gradiente en el entrenamiento es muy pequeño (gradiente desvanecido) o demasiado grande (gradiente explosivo), el entrenamiento del modelo se vuelve ineficaz.\n",
        "\n",
        "**Soluciones comunes** incluyen el uso de arquitecturas especializadas como las **LSTM** o las **GRU**, que están diseñadas para mitigar el problema del gradiente desvanecido. Además, técnicas como la **normalización de gradientes** (gradient clipping) también pueden ayudar a prevenir el gradiente explosivo.\n",
        "\n",
        "#### 3.4. **Overfitting y Underfitting**\n",
        "El sobreajuste (overfitting) ocurre cuando un modelo tiene un rendimiento excelente en el conjunto de entrenamiento, pero falla al generalizar con datos nuevos. Por otro lado, el subajuste (underfitting) se produce cuando un modelo no puede capturar la complejidad de los datos de entrenamiento, y por lo tanto, tiene un rendimiento deficiente.\n",
        "\n",
        "El equilibrio entre overfitting y underfitting puede lograrse con una arquitectura adecuada, una correcta elección de hiperparámetros y el uso de **validación cruzada** para evitar el sobreajuste.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **El Gradiente Descendente y el Entrenamiento**\n",
        "\n",
        "#### 4.1. **Algoritmo del Gradiente Descendente**\n",
        "El algoritmo del **gradiente descendente** es el método más comúnmente utilizado para optimizar modelos de redes neuronales. Su objetivo es minimizar la función de pérdida ajustando los pesos de la red mediante pequeñas actualizaciones en la dirección opuesta al gradiente de la función de pérdida con respecto a los pesos.\n",
        "\n",
        "- **Tasa de Aprendizaje (Learning Rate)**: Define qué tan grandes son los pasos en cada iteración. Un valor demasiado alto puede hacer que el modelo no converja, mientras que un valor muy bajo puede hacer que el modelo tarde demasiado en entrenarse.\n",
        "- **Gradiente Descendente Estocástico (SGD)**: En lugar de calcular el gradiente para todo el conjunto de datos (gradiente descendente completo), se calcula el gradiente para un pequeño subconjunto de datos, lo que acelera el proceso de entrenamiento.\n",
        "\n",
        "#### 4.2. **Problemas de Convergencia**\n",
        "El proceso de optimización del gradiente descendente no está exento de desafíos. Entre los problemas más comunes están:\n",
        "\n",
        "- **Estancamiento en óptimos locales**: Algunas veces el gradiente descendente puede converger en mínimos locales, evitando que el modelo alcance un rendimiento óptimo.\n",
        "- **Desvanecimiento del gradiente**: En redes profundas, las actualizaciones de los pesos en las capas iniciales pueden ser insignificantes, lo que reduce la capacidad de la red para aprender características importantes en las capas más profundas.\n",
        "  \n",
        "#### 4.3. **Mejoras en el Gradiente Descendente**\n",
        "Existen varias mejoras del gradiente descendente estándar que ayudan a acelerar el entrenamiento y mejorar la convergencia:\n",
        "\n",
        "- **Adam (Adaptive Moment Estimation)**: Es uno de los algoritmos de optimización más utilizados. Combina lo mejor de RMSProp y el gradiente descendente estocástico con momentum, ajustando dinámicamente la tasa de aprendizaje de cada parámetro.\n",
        "  \n",
        "- **RMSProp**: Divide la tasa de aprendizaje por una media móvil de las magnitudes recientes del gradiente para acelerar el entrenamiento en redes neuronales profundas.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "MruXMguo5J1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 5. Arquitecturas de Redes Neuronales Profundas\n",
        "\n",
        "5.1. **Perceptrón Multicapa (MLP - Multilayer Perceptron)**\n",
        "   - **Descripción**: Es la forma más básica de una red neuronal profunda. Consiste en capas completamente conectadas (también conocidas como capas densas o fully connected layers), donde cada neurona en una capa está conectada a todas las neuronas de la capa siguiente.\n",
        "   - **Usos**: Problemas de clasificación y regresión.\n",
        "   - **Problemas que resuelve**: Clasificación de datos estructurados (tabulares), predicciones en series temporales simples.\n",
        "   - **Limitaciones**: No captura bien patrones espaciales o secuenciales en los datos, como en imágenes o texto.\n",
        "\n",
        "    **Ejemplo de uso**: Clasificación de imágenes simples o predicción de precios de casas.\n",
        "\n",
        "---\n",
        "\n",
        "5.2. **Redes Neuronales Convolucionales (CNN - Convolutional Neural Networks)**\n",
        "   - **Descripción**: Las CNN están diseñadas específicamente para procesar datos en forma de imágenes o secuencias de imágenes. Utilizan capas convolucionales que permiten detectar patrones locales en las imágenes, como bordes, texturas o formas.\n",
        "   - **Usos**: Procesamiento de imágenes, video, reconocimiento de patrones visuales.\n",
        "   - **Problemas que resuelve**: Clasificación de imágenes (e.g. detectar gatos en imágenes), segmentación de imágenes (e.g. separación de objetos en imágenes), reconocimiento de objetos en video.\n",
        "   - **Limitaciones**: No son eficientes para secuencias largas de datos no espaciales (e.g. texto).\n",
        "\n",
        "    **Ejemplo de uso**: Clasificación de imágenes de dígitos (MNIST), detección de objetos en imágenes.\n",
        "\n",
        "---\n",
        "\n",
        "5.3. **Redes Neuronales Recurrentes (RNN - Recurrent Neural Networks)**\n",
        "   - **Descripción**: Estas redes están diseñadas para trabajar con datos secuenciales, como texto o series temporales. Tienen una memoria interna que les permite almacenar información sobre pasos previos en la secuencia.\n",
        "   - **Usos**: Modelado de texto, series temporales, traducción automática, predicción de secuencias.\n",
        "   - **Problemas que resuelve**: Predicción de la siguiente palabra en una frase, análisis de sentimiento en texto, predicción del clima en series temporales.\n",
        "   - **Limitaciones**: Pueden tener problemas con secuencias largas debido al problema de \"vanishing gradients\" (gradientes que desaparecen), lo que dificulta el aprendizaje de dependencias a largo plazo.\n",
        "\n",
        "    **Ejemplo de uso**: Generación de texto, predicción de precios en finanzas.\n",
        "\n",
        "---\n",
        "\n",
        "5.4. **Redes Neuronales de Memoria a Largo Plazo (LSTM - Long Short-Term Memory)**\n",
        "   - **Descripción**: Son una mejora de las RNN, diseñadas para superar los problemas de \"vanishing gradients\". Las LSTM tienen celdas especiales que pueden recordar información durante períodos largos de tiempo.\n",
        "   - **Usos**: Procesamiento de secuencias, como predicción de series temporales, procesamiento de texto, traducción automática.\n",
        "   - **Problemas que resuelve**: Dependencias a largo plazo en secuencias, como relaciones de palabras en oraciones complejas.\n",
        "   - **Limitaciones**: Pueden ser más lentas de entrenar debido a su complejidad en comparación con las RNN estándar.\n",
        "\n",
        "    **Ejemplo de uso**: Traducción automática (Google Translate), predicción de ventas.\n",
        "\n",
        "---\n",
        "\n",
        "5.5. **Redes de Transformadores (Transformers)**\n",
        "   - **Descripción**: Una de las arquitecturas más avanzadas, que no depende de la recurrencia como las RNN o LSTM, sino que utiliza mecanismos de \"self-attention\" (autoatención) para procesar datos secuenciales.\n",
        "   - **Usos**: Modelado de secuencias largas, procesamiento de texto, comprensión de lenguaje natural.\n",
        "   - **Problemas que resuelve**: Traducción automática, generación de texto, comprensión de texto.\n",
        "   - **Limitaciones**: Requieren mucha capacidad computacional y grandes cantidades de datos para entrenar eficientemente.\n",
        "\n",
        "    **Ejemplo de uso**: Modelos de lenguaje avanzado como GPT-3, BERT para tareas de NLP (Natural Language Processing).\n",
        "\n",
        "---\n",
        "\n",
        "5.6. **Autoencoders**\n",
        "   - **Descripción**: Son redes neuronales diseñadas para aprender una representación comprimida de los datos (codificación) y luego reconstruirlos desde esa representación comprimida (decodificación). Su objetivo es aprender una representación de los datos más eficiente.\n",
        "   - **Usos**: Compresión de datos, detección de anomalías, reducción de dimensionalidad.\n",
        "   - **Problemas que resuelve**: Detección de anomalías en datos (como en transacciones fraudulentas), reducción de ruido en imágenes, reducción de dimensionalidad para visualización.\n",
        "   - **Limitaciones**: No es útil para tareas de clasificación o regresión directamente.\n",
        "\n",
        "    **Ejemplo de uso**: Eliminación de ruido en imágenes, compresión de datos en imágenes de alta resolución.\n",
        "\n",
        "---\n",
        "\n",
        "### Comparación de las Arquitecturas\n",
        "\n",
        "| Arquitectura  | Usos Principales | Ventajas | Limitaciones |\n",
        "|---------------|------------------|----------|--------------|\n",
        "| **MLP**       | Clasificación, regresión | Simplicidad, fácil de implementar | Ineficiente para datos espaciales o secuenciales |\n",
        "| **CNN**       | Procesamiento de imágenes, video | Eficiente en imágenes y datos espaciales | No es adecuada para secuencias largas o datos textuales |\n",
        "| **RNN**       | Texto, series temporales | Capacidad para modelar secuencias | Dificultad con secuencias largas (vanishing gradients) |\n",
        "| **LSTM**      | Texto, secuencias largas | Maneja dependencias a largo plazo | Complejidad computacional |\n",
        "| **Transformers** | Procesamiento de texto, modelado de secuencias largas | Maneja secuencias largas sin recurrencia | Alto costo computacional |\n",
        "| **Autoencoders** | Reducción de dimensionalidad, detección de anomalías | Representación comprimida eficiente | No es adecuado para clasificación/regresión |\n",
        "\n",
        "---\n",
        "\n",
        "### Problemas Típicos que Resuelven\n",
        "\n",
        "- **Clasificación**: ¿Qué categoría pertenece un dato? Ejemplos: detección de objetos, análisis de sentimiento, reconocimiento de dígitos.\n",
        "- **Regresión**: Predicción de un valor continuo. Ejemplo: predicción de precios de viviendas.\n",
        "- **Segmentación**: División de una imagen en diferentes objetos o regiones. Ejemplo: segmentación de imágenes médicas.\n",
        "- **Predicción de Secuencias**: Predicción del siguiente elemento en una secuencia. Ejemplo: predicción de palabras en texto, predicción de series temporales.\n",
        "- **Reducción de Dimensionalidad**: Compresión de datos para hacerlos más manejables. Ejemplo: reducción de características para visualización."
      ],
      "metadata": {
        "id": "EX7K_rB96Bzq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 5. **Resumen Final**\n",
        "\n",
        "El aprendizaje profundo ha cambiado radicalmente la manera en que las máquinas pueden aprender y generalizar a partir de grandes cantidades de datos. Las redes neuronales profundas son increíblemente poderosas, pero también presentan desafíos, como la necesidad de grandes cantidades de datos y poder computacional, y la complejidad de ajustar correctamente los hiperparámetros. Librerías como **TensorFlow**, **Keras**, y **PyTorch** proporcionan herramientas avanzadas para entrenar y desplegar estos modelos, mientras que algoritmos de optimización como **Adam** y técnicas como la regularización permiten obtener modelos más eficientes y precisos.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Lecturas Adicionales**\n",
        "\n",
        "- **Ian Goodfellow, Yoshua Bengio, Aaron Courville**: *Deep Learning* (MIT Press).\n",
        "- **Documentación de TensorFlow**: https://www.tensorflow.org/\n",
        "- **Documentación de PyTorch**: https://pytorch.org/"
      ],
      "metadata": {
        "id": "a-4oj7pB6CWL"
      }
    }
  ]
}